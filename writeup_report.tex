
\documentclass[a4paper, 11pt, DIV=14]{scrartcl}

\author{Bastian Dittmar}
\date{\today}
\subject{Self Driving Car Nano Degree}
\title{Advanced Lane Finding}

\usepackage{wrapfig, floatrow}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{csquotes}
\usepackage{hyperref}
\hypersetup{
     colorlinks   = true,
     citecolor    = blue,
     linkcolor    = blue
}
\usepackage{amsmath}
\usepackage{listings}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.}}

\usepackage[style=authoryear, citestyle=authoryear-icomp, giveninits=true, autolang=hyphen, hyperref=true, minbibnames=3, dashed=false, doi=false, isbn=false, url=false, sorting=nyt, backend=biber]{biblatex}
\setlength{\bibitemsep}{0.5\baselineskip}
\addbibresource{library.bib}

\pagestyle{plain}
\begin{document}
\maketitle


\section{Introduction}
In this project, a software pipeline is implemented to identify the lane boundaries in a video from a front-facing camera on a car. The code to solve this project is provided in a jupyter notebook.

The goals and steps of this project are the following:
\begin{enumerate}
\item Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.
\item Apply a distortion correction to raw images.
\item Use color transforms, gradients, etc., to create a thresholded binary image.
\item Apply a perspective transform to rectify binary image (birds-eye view).
\item Detect lane pixels and fit to find the lane boundary.
\item Determine the curvature of the lane and vehicle position with respect to center.
\item Warp the detected lane boundaries back onto the original image.
\item Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.
\end{enumerate}


\section{Camera Calibration}
\label{sec:camera_calibration}
The code for this step is contained in code cells 2 and 3 of the IPython notebook located in ``./AdvancedLaneLines.ipynb''

I start by preparing "object points", which will be the (x, y, z) coordinates of the chessboard corners in the world. Here I am assuming the chessboard is fixed on the (x, y) plane at z=0, such that the object points are the same for each calibration image. Thus, objp is just a replicated array of coordinates, and objpoints will be appended with a copy of it every time I successfully detect all chessboard corners in a test image. imgpoints will be appended with the (x, y) pixel position of each of the corners in the image plane with each successful chessboard detection.

I then used the output objpoints and imgpoints to compute the camera calibration and distortion coefficients using the cv2.calibrateCamera() function. I applied this distortion correction to the test image using the cv2.undistort() function. The result is depicted in figure \ref{fig:camera_calibration}.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/chessboard10_distorted.jpg}
        \caption{Original chessboard image}
        \label{fig:chessboard_original}
    \end{subfigure}\quad
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/chessboard10_undistorted.jpg}
        \caption{Undistorted warped into image plane}
        \label{fig:chessboard_undistorted}
    \end{subfigure} 
    \caption{Example chessboard images used for camera calibration}
    \label{fig:camera_calibration}
\end{figure}

\section{Pipeline (single images)}

\subsection{Distortion Correction}
The first step is to correct the image for distortion. I used the camera matrix and distortion coefficients I obtained from the camera calibration in section \ref{sec:camera_calibration} with the function cv2.undistort(). The result is shown in figure \ref{fig:test02_undistort}.

\begin{figure}
\includegraphics[width=.925\textwidth]{images/preprocess_test02__undistort.jpg}
\caption{Undistorted test image test02.jpg}
\label{fig:test02_undistort}
\end{figure}

\subsection{Perspective Transformation}
The second step of my pipeline is the perspective transformation of the image into a bird's view. I chose to perform this step before thresholding the image as I found the result to be better. In particular for gradient thresholds which use a kernel of constant size over the image, the distance of an object to the camera matters in a perspective view. Lines that are further away will appear thinner and would require a smaller kernel size than lines in the front for similar results. The bird's view perspective on the other hand transforms the lines to be more or less equally thick along the whole image.

To find a proper transformation 4 points on the host lane lines were chosen and then transformed into a square. The result is depicted in figure \ref{fig:birds_view} and the code in cell ``Perspective Transform''. The transformation from warping this image is used in the pipeline to transform all images into the bird's view. This allows us to measure curvature of the lane later on.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/straight_lines.jpg}
    \end{subfigure}\quad
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/straight_lines_warped.jpg}
    \end{subfigure} 
    \caption{Bird's view transformation: 4 points on the lane lines were choses that are in shape of a rectangle in world coordinates (assuming the world is flat)}
    \label{fig:birds_view}
\end{figure}

\subsection{Segmentation}

In the next step the image will be segmented using a variation of thresholding algorithms. Result is a binary image with active pixel that we suspect to be lane pixel. The segmentation is done in code cells ``Thresholding Functions'' and ``Image Preprocessing''.

\subsubsection*{Color Threshold}
The first part of the segmentation is a color threshold that tries to identify bright yellow and white objects in the image as those are the lane colors. To account for variations in illumination like shadows I transformed the Image from the BGR color space into the HSV color space. In the HSV color spaces shadows will have a low V-value. The color itself is selected by hue and saturation. Yellow has high saturation in HSV while white has low saturation. The hue for yellow is rather specific around a value of 20 while white is no specific hue. For yellow i used the HSV-thresholds (0-40), (80-255) and (200-255). For white the HSV-thresholds (20-255), (0-80) and (200-255) seemed to work. The thresholds are first applied on the respective channel separately and then the results are combined with logical-and. Thus a pixel has to fall in all threshold ranges to be selected. This is done in function combined\_channel\_treshold() in cell ``Thresholding Functions''. The results of this step can be seen in figure \ref{fig:color_threshold}.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/preprocess_test02__hsv_yellow.jpg}
    \end{subfigure}\quad
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/preprocess_test02__hsv_white.jpg}
    \end{subfigure} 
    \caption{Color thresholds for yellow lines (left) and white lines (right) on test02.jpg.}
    \label{fig:color_threshold}
\end{figure}

\subsubsection*{Gradient Threshold}
In addition to the color thresholds I chose to apply a gradient threshold using a Sobel operator in x-direction. This will mostly find vertical edges in the image. While the directional gradient sounds promising in theory as well, I found it too noisy to work with in practice. For the gradient in x-direction I chose the HLS color space and applied the gradient to the L- and S-channel which pick up lines quite well regardless of the illumination. For the former I used a threshold of (40-255) and for the latter a threshold of (25-255). The results are shown in figure \ref{fig:gradient_threshold}. To further reduce noise a median filter with kernel size 9.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/preprocess_test02__glx.jpg}
    \end{subfigure}\quad
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/preprocess_test02__gsx.jpg}
    \end{subfigure} 
    \caption{Gradient thresholds in x-direction i√≥n the L-channe (left) and S-channel (right) on test02.jpg.}
    \label{fig:gradient_threshold}
\end{figure}

\subsubsection*{Combining Thresholds}
The final binary image is a logical-or combination of all 4 thresholding procedures, meaning any pixel that is active in at least one of the threshold images will be picked. The result is shown in figure \ref{fig:threshold_final} on test02.jpg where the lane is clearly visible. While test02.jpg is easy, it demonstrates the participation of all 4 thresholds quite well.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/preprocess_test02__combined-OR.jpg}
	\caption{or-combined thresholds}
	\label{fig:threshold_combined}
    \end{subfigure}\quad
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/preprocess_test02__warped.jpg}
    \caption{original image in bird's view}
    \end{subfigure} 
    \caption{The combination of all threshold images of the undistorted image test02.jpg in bird's view.}
    \label{fig:threshold_final}
\end{figure}

\subsection{Lane Detection}
The lane detection algorithm is implemented in cell ``Lane Detection'' and uses the thresholded image in figure \ref{fig:threshold_combined}. There are two different algorithms to detect the left and right lane lines that differ in how they estimate the search areas where they look for lane pixels.

\subsubsection*{Sliding Window}
The sliding window approach starts with a histogram of the bottom half of the image. It then calculates the history maxima left and right of the image's center. The x-coordinates are starting positions. For each position 9 windows will be stacked on top of each other. All pixels within a window will count as lane pixel and the current window position will be centered around the mean x-value of the lane pixels if there were more than 50 pixels. The next window's position is a linear extrapolation of the previous two positions. The resulting 18 sliding windows are depicted in figure \ref{fig:sliding_windows}. The lane lines are then estimated by fitting a second order polynom to the located lane pixels using numpy's polyfit() function. The polynomials are colored yellow.

\subsubsection*{Polynomial Search Arrea}
If there already is information about previous lane positions another algorithm can be used. Instead of sliding windows from the bottom to the top, tracking lane pixels, the search area is defined by the previous lane polynomial and a width to either side. Any pixel in that area will be considered a lane pixel. The search area is shown in figure \ref{fig:polynomial_search}. The lane estimation then is the same as in the sliding windows approach using numpy's polyfit().


\begin{figure}[h]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/lane_detection_test02_01.jpg}
	\caption{Slidng windows}
	\label{fig:sliding_windows}
    \end{subfigure}\quad
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/lane_detection_test02_02.jpg}
    \caption{polynomial search area}
    \label{fig:polynomial_search}
    \end{subfigure} 
    \caption{The two different search approaches for lane detection. Search areas are marked green, estimated lane position in yellow.}  
    \label{fig:lane_detection}
\end{figure}

\subsubsection*{Curve Radius Estimation}
After the lane pixels have been detected and are fitted to a polynomial we measure the curvature. This can be done with there formula

$$ R = \frac{(1+(2Ay_0 + B)^2)^{1.5}}{|2A|}$$

where the curve is $f(y)= Ay^2 + By + C$ and $y_0$ is the position where the radius is measured. In our case we want to know the curvature at the car's position so we choose the bottom of the image. The resulting radius would be in pixel dimensions. As we are interested in something that relates to world coordinates, we scale the coordinate system before fitting the polynomial as well as $y_0$. For the scaling factors I googled the width of a lane (3.7 m) and he distance between dashed lines (9 m) and the length of one dash (3 m). Mapping those parameters to a transformed image in bird's view and measuring the pixels of those lengths I estimated a scaling of

 $$s_x = \frac{3.7}{760} \frac{m}{px}\;\;\; \text{ and } \;\;\;s_y = \frac{30}{700} \frac{m}{px}.$$
 
\subsubsection*{Lane Tracking}
To decide which of the lane detection approaches to use and to increase robustness we keep track of both lane line curves for the last 10 frames. Everytime we fit a new polynomial to lane pixels we make a sanity check of the line's distance to each other at the car's position. If it varies more than 30 cm from the expected distance of 3.7 m, the lane will be rejected. Otherwise both lines will replace the oldest lines in a list of the last 10 lane lines. The actual lane curve that is used for measuring the curvature or being displayed is an average of the last 10 curves. That makes the behavior not only more robust but also more visually pleasing as it prevents jumping lines from frame to frame.

\subsubsection*{Lane Projection}
The last step is to project the lane curves back into the original image. This is done by using the inverse Matrix from the bird's view projection and overlaying it with the original image. The result can be seen in figure \ref{fig:lane_projection}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.925\textwidth]{images/lane_projection_test02_01.jpg}

    \caption{Projection of the detected lane onto the original image test02.jpg.}
    \label{fig:lane_projection}
\end{figure}

\section{Pipeline (video)}
The pipeline has also been tested on the project video. The result can be found at project\_video\_out.mp4 or with debug information at project\_video\_out\_debug.mp4. 

\section{Discussion}
The main issue for this project was to find the right combination of threshold parameters. Currently it feels like they are quite project specific to the test set of images and the video. Something adaptive would be nicer that estimates optimal thresholds per frame or even per line for something like horizontal thresholds. The lighting conditions might be a good start for the adaptive thresholding. I tried briefly to work with the in openCV built-in adaptive thresholds but the results were not pleasing.

My other problem was a sanity check. The curvature values are jumping a lot because the lane lines often only cover part of the vertical space and they vary in width. This leads to a lot of noise in curve fitting. I tried to monitor variations in the polynomial coefficients, parallelity of the lane lines but nothing was really useful in the sense, that large deviations would match unpleasing visual results.

When the car is jumping and the contrast of road surface to lane lines is weak, the algorithm fails. This is indicated by red lanes in the video and depicted in figure \ref{fig:sanity_check}. The lanes are rejected because they are too far apart (4.6 m). But if we look at the curves this is not the only problem. The result is far from reality because the left lane detection is too short.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.925\textwidth]{images/lane_projection_sanity_01.jpg}

    \caption{Red lanes indicate that the lane detection was rejected for this frame..}
    \label{fig:sanity_check}
\end{figure} 

\printbibliography
\end{document}
